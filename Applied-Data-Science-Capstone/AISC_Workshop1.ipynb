{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AISC_Workshop1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JasonLUrquhart/Applied-Data-Science-Capstone/blob/master/AISC_Workshop1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UarGd4mvYj3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This notebook is part of the workshop \"modern natural language processing\" run \n",
        "# by Aggregate Intellect Inc. (https://ai.science), and is released under \n",
        "# 'Creative Commons Attribution-NonCommercial-ShareAlike CC BY-NC-SA\" license. \n",
        "# This material can be altered and distributed for non-commercial use with \n",
        "# reference to Aggregate Intellect Inc. as the original owner, and any material \n",
        "# generated from it must be released under similar terms. \n",
        "# (https://creativecommons.org/licenses/by-nc-sa/4.0/)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebxVMIWLBZHj",
        "colab_type": "text"
      },
      "source": [
        "# Huggingface's BERT\n",
        "\n",
        "Huggingface, a company focused on social chatbots, provides Pytorch implementations of several cutting edge models!\n",
        "\n",
        "We'll be using[ their implementation of BERT](https://github.com/huggingface/pytorch-pretrained-BERT) for this workshop.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYZGRbrsELLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! git clone https://github.com/huggingface/pytorch-pretrained-BERT.git\n",
        "% cd pytorch-pretrained-BERT\n",
        "! pip install .\n",
        "%cd .."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUB-HicyEPRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import logging\n",
        "import math\n",
        "import collections\n",
        "from io import open\n",
        "import pickle\n",
        "import pprint\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)\n",
        "\n",
        "from tqdm import tqdm, trange, tqdm_notebook\n",
        "from time import sleep\n",
        "\n",
        "from pytorch_pretrained_bert import BertForQuestionAnswering\n",
        "from pytorch_pretrained_bert.tokenization import BasicTokenizer, whitespace_tokenize, BertTokenizer\n",
        "from pytorch_pretrained_bert.file_utils import WEIGHTS_NAME, CONFIG_NAME\n",
        "from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iKu63W4JFGw",
        "colab_type": "text"
      },
      "source": [
        "### Import some functions from Huggingface's github\n",
        "\n",
        "The source we are loading as a module provides classes to represent training examples and model features, as well as data preprocessing functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMZynNR_DNux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib.request\n",
        "import imp\n",
        "\n",
        "# URL to the raw .py file used to read and convert SQuAD \n",
        "# dataset to feature vectors\n",
        "source = 'https://raw.githubusercontent.com/huggingface/pytorch-pretrained-BERT/' + \\\n",
        "         'master/examples/run_squad_dataset_utils.py'\n",
        "modulesource = urllib.request.urlopen(source).read()\n",
        "\n",
        "def makemodule(modulesource,sourcestr,modname=None):\n",
        "    if not modname: modname = 'newmodulename'\n",
        "    codeobj = compile(modulesource, sourcestr, 'exec')\n",
        "    newmodule = imp.new_module(modname)\n",
        "    exec(codeobj,newmodule.__dict__)\n",
        "    return newmodule\n",
        "  \n",
        "data_functs = makemodule(modulesource, source)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFlGphcWx1n8",
        "colab_type": "text"
      },
      "source": [
        "### Create a tokenizer to process raw text\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7F2KTtRxziS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39G2zHzUMhvj",
        "colab_type": "text"
      },
      "source": [
        "# The SQuAD Dataset\n",
        "\n",
        "Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage.\n",
        "\n",
        "You can find more information about the dataset, and approaches to reading comprehension [ here!](https://rajpurkar.github.io/SQuAD-explorer/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al-FgiqJyAD3",
        "colab_type": "text"
      },
      "source": [
        "### Download train/test examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6uq8vbwztMGl",
        "colab": {}
      },
      "source": [
        "!wget 'https://drive.google.com/uc?id=1OBjbuy9lIxcKrnqAq6n-m8S7T_GBw2h0' -O 'train-v1.1.json'\n",
        "!wget 'https://drive.google.com/uc?id=1TtsI3_Jm2OQuJ_nOi2_KoBP9RdcRAs9r' -O 'eval-v1.1.json'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7t8U2nT9L8Mc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = 'train-v1.1.json'\n",
        "train_examples = data_functs.read_squad_examples(\n",
        "                   filename, \n",
        "                   True, \n",
        "                   False)\n",
        "\n",
        "train_examples = data_functs.read_squad_examples(\n",
        "                   filename, \n",
        "                   True, \n",
        "                   False)\n",
        "\n",
        "eval_filename = 'eval-v1.1.json'\n",
        "eval_examples = data_functs.read_squad_examples(\n",
        "                  eval_filename,\n",
        "                  False,\n",
        "                  False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjxHXstoyKz9",
        "colab_type": "text"
      },
      "source": [
        "### Some notes on tokenizing\n",
        "\n",
        "BERT Uses WordPiece Tokenization, which helps handle a broad range of words outside of the vocabulary. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3yshdPDpdUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_examples[8635].question_text)\n",
        "print(tokenizer.tokenize(train_examples[8635].question_text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AJCklr6YvSS",
        "colab_type": "text"
      },
      "source": [
        "## Exploring the SQuAD dataset\n",
        "\n",
        "We've defined a type to represent our training examples.\n",
        "\n",
        "It contains:\n",
        "\n",
        "1.   the question and answer text\n",
        "2.   the tokenized document we need to comprehend\n",
        "3.   positions of the answer in the document.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaE657HEbU_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Anatomy of a SQuAD example\n",
        "ex = train_examples[0]\n",
        "print(f\"Document: {ex.doc_tokens}\")\n",
        "print(f\"Question: {ex.question_text}\")\n",
        "print(f\"Answer: {ex.orig_answer_text}\")\n",
        "print(f\"Answer token position range: {(ex.start_position, ex.end_position)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw3Om5sliyTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.hist([len(ex.doc_tokens) for ex in train_examples])\n",
        "plt.title(\"Length SQuAD of Training Documents (Tokenized)\")\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuUH1Kw6xCBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist([len(tokenizer.tokenize(ex.question_text)) for ex in train_examples])\n",
        "plt.title(\"Length SQuAD of Training Questions (Tokenized)\")\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RR0hDYREXbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_seq_length=324\n",
        "doc_stride=128\n",
        "max_query_length=64\n",
        "predict_batch_size=128\n",
        "local_rank = -1\n",
        "n_best_size=20\n",
        "max_answer_length=30\n",
        "is_training=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssttrMGGJCrn",
        "colab_type": "text"
      },
      "source": [
        "# Making Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgZ4sb1ub3Zk",
        "colab_type": "text"
      },
      "source": [
        "## Importing our Fine-Tuned BERT for SQuAD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDSzhouJJryF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hre-JJ_0Quj4",
        "colab_type": "text"
      },
      "source": [
        "## Setup to execute the model on our GPU and Look at the Architecture\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DeYve0Jo86R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_gpu = torch.cuda.device_count()\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJOBTfAXRLcK",
        "colab_type": "text"
      },
      "source": [
        "## Converting text to input features\n",
        "\n",
        "We need to transform our tokens into numeric features for our network to consume!\n",
        "\n",
        "Tokens are mapped to integers, which are mapped to embeddings in the first layer of our network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMbdQe5aTQp0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "help(data_functs.convert_examples_to_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQMF1YmvJQg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_features = data_functs.convert_examples_to_features(\n",
        "            examples=eval_examples,\n",
        "            tokenizer=tokenizer,\n",
        "            max_seq_length=max_seq_length,\n",
        "            doc_stride=doc_stride,\n",
        "            max_query_length=max_query_length,\n",
        "            is_training=is_training)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Esg17nRYan18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_features = eval_features[:1000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt5ch5Vy1K1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"***** Running predictions *****\")\n",
        "print(f\"Num orig examples = {len(eval_examples)}\")\n",
        "print(f\"Num split examples = {len(eval_features)}\")\n",
        "print(f\"Batch size = {predict_batch_size}\")\n",
        "\n",
        "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
        "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
        "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
        "all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
        "\n",
        "\n",
        "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_example_index)\n",
        "\n",
        "# Run prediction for full data\n",
        "eval_sampler = SequentialSampler(eval_data)\n",
        "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=predict_batch_size)\n",
        "\n",
        "model.eval()\n",
        "all_results = []\n",
        "for input_ids, input_mask, segment_ids, example_indices in \\\n",
        "      tqdm_notebook(eval_dataloader, desc=\"Evaluating\", disable=local_rank not in [-1, 0]):\n",
        "    sleep(0.0001) # sleep to help tqdm progress bar behave\n",
        "\n",
        "    input_ids = input_ids.to(device)\n",
        "    input_mask = input_mask.to(device)\n",
        "    segment_ids = segment_ids.to(device)\n",
        "    with torch.no_grad():\n",
        "        batch_start_logits, batch_end_logits = model(input_ids, segment_ids, input_mask)\n",
        "        for i, example_index in enumerate(example_indices):\n",
        "            start_logits = batch_start_logits[i].detach().tolist()\n",
        "            end_logits = batch_end_logits[i].detach().tolist()\n",
        "            eval_feature = eval_features[example_index.item()]\n",
        "            unique_id = int(eval_feature.unique_id)\n",
        "            all_results.append(data_functs.RawResult(\n",
        "                                   unique_id=unique_id,\n",
        "                                   start_logits=start_logits,\n",
        "                                   end_logits=end_logits)\n",
        "                              )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_bFbjGSb9CB",
        "colab_type": "text"
      },
      "source": [
        "## Raw Results\n",
        "\n",
        "Our model produces two sets of logits representing the predicted Start and End positions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOtGrQL6ceYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pprint.pprint(all_results[0].__repr__())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5flfnOAqY4kQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pprint.pprint(np.argmax(all_results[0].start_logits))\n",
        "pprint.pprint(np.argmax(all_results[0].end_logits))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT1_Yk5ES_1P",
        "colab_type": "text"
      },
      "source": [
        "## Transform our output logits into responses\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEXX6ko6TAbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "help(data_functs.write_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9JBd4Or7YxV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_functs.write_predictions(\n",
        "    eval_examples, \n",
        "    eval_features, \n",
        "    all_results,\n",
        "    n_best_size, \n",
        "    max_answer_length,\n",
        "    False, \n",
        "    \"predictions.json\",\n",
        "    \"n_best.json\", \n",
        "    \"null_log_odds\", \n",
        "    False,\n",
        "    False, \n",
        "    0.0\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhd8Y8YRUZKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(eval_examples[0].__repr__())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUYLa6dhrGhF",
        "colab_type": "text"
      },
      "source": [
        "## Create a map of question IDs to associate results "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0IECMHSEQhb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "questions = {}\n",
        "for i in eval_examples:\n",
        "  if i.qas_id not in questions:\n",
        "    questions[i.qas_id] = i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-NrgXzUt14K",
        "colab_type": "text"
      },
      "source": [
        "## Explore our results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baJK5SpWsFbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = json.load(open('predictions.json','r'))\n",
        "\n",
        "for i in data:\n",
        "  print(f\"Question: {questions[i].question_text}\\nDocument:{questions[i].doc_tokens}\\nPredicted Answer: {data[i]}\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "casrJ3C3v8ZW",
        "colab_type": "text"
      },
      "source": [
        "## Understanding how we extract features from text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcwZKCktsJHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\" Load SQuAD dataset. \"\"\"\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import collections\n",
        "from io import open\n",
        "\n",
        "from pytorch_pretrained_bert.tokenization import BasicTokenizer, whitespace_tokenize\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class SquadExample(object):\n",
        "    \"\"\"\n",
        "    A single training/test example for the Squad dataset.\n",
        "    For examples without an answer, the start and end position are -1.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 qas_id,\n",
        "                 question_text,\n",
        "                 doc_tokens,\n",
        "                 orig_answer_text=None,\n",
        "                 start_position=None,\n",
        "                 end_position=None,\n",
        "                 is_impossible=None):\n",
        "        self.qas_id = qas_id\n",
        "        self.question_text = question_text\n",
        "        self.doc_tokens = doc_tokens\n",
        "        self.orig_answer_text = orig_answer_text\n",
        "        self.start_position = start_position\n",
        "        self.end_position = end_position\n",
        "        self.is_impossible = is_impossible\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.__repr__()\n",
        "\n",
        "    def __repr__(self):\n",
        "        s = \"\"\n",
        "        s += \"qas_id: %s\" % (self.qas_id)\n",
        "        s += \", question_text: %s\" % (\n",
        "            self.question_text)\n",
        "        s += \", doc_tokens: [%s]\" % (\" \".join(self.doc_tokens))\n",
        "        if self.start_position:\n",
        "            s += \", start_position: %d\" % (self.start_position)\n",
        "        if self.end_position:\n",
        "            s += \", end_position: %d\" % (self.end_position)\n",
        "        if self.is_impossible:\n",
        "            s += \", is_impossible: %r\" % (self.is_impossible)\n",
        "        return s\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 unique_id,\n",
        "                 example_index,\n",
        "                 doc_span_index,\n",
        "                 tokens,\n",
        "                 token_to_orig_map,\n",
        "                 token_is_max_context,\n",
        "                 input_ids,\n",
        "                 input_mask,\n",
        "                 segment_ids,\n",
        "                 start_position=None,\n",
        "                 end_position=None,\n",
        "                 is_impossible=None):\n",
        "        self.unique_id = unique_id\n",
        "        self.example_index = example_index\n",
        "        self.doc_span_index = doc_span_index\n",
        "        self.tokens = tokens\n",
        "        self.token_to_orig_map = token_to_orig_map\n",
        "        self.token_is_max_context = token_is_max_context\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.start_position = start_position\n",
        "        self.end_position = end_position\n",
        "        self.is_impossible = is_impossible\n",
        "\n",
        "\n",
        "def read_squad_examples(input_file, is_training, version_2_with_negative):\n",
        "    \"\"\"Read a SQuAD json file into a list of SquadExample.\"\"\"\n",
        "    with open(input_file, \"r\", encoding='utf-8') as reader:\n",
        "        input_data = json.load(reader)[\"data\"]\n",
        "\n",
        "    def is_whitespace(c):\n",
        "        if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    examples = []\n",
        "    for entry in input_data:\n",
        "        for paragraph in entry[\"paragraphs\"]:\n",
        "            paragraph_text = paragraph[\"context\"]\n",
        "            doc_tokens = []\n",
        "            char_to_word_offset = []\n",
        "            prev_is_whitespace = True\n",
        "            for c in paragraph_text:\n",
        "                if is_whitespace(c):\n",
        "                    prev_is_whitespace = True\n",
        "                else:\n",
        "                    if prev_is_whitespace:\n",
        "                        doc_tokens.append(c)\n",
        "                    else:\n",
        "                        doc_tokens[-1] += c\n",
        "                    prev_is_whitespace = False\n",
        "                char_to_word_offset.append(len(doc_tokens) - 1)\n",
        "\n",
        "            for qa in paragraph[\"qas\"]:\n",
        "                qas_id = qa[\"id\"]\n",
        "                question_text = qa[\"question\"]\n",
        "                start_position = None\n",
        "                end_position = None\n",
        "                orig_answer_text = None\n",
        "                is_impossible = False\n",
        "                if is_training:\n",
        "                    if version_2_with_negative:\n",
        "                        is_impossible = qa[\"is_impossible\"]\n",
        "                    if (len(qa[\"answers\"]) != 1) and (not is_impossible):\n",
        "                        raise ValueError(\n",
        "                            \"For training, each question should have exactly 1 answer.\")\n",
        "                    if not is_impossible:\n",
        "                        answer = qa[\"answers\"][0]\n",
        "                        orig_answer_text = answer[\"text\"]\n",
        "                        answer_offset = answer[\"answer_start\"]\n",
        "                        answer_length = len(orig_answer_text)\n",
        "                        start_position = char_to_word_offset[answer_offset]\n",
        "                        end_position = char_to_word_offset[answer_offset + answer_length - 1]\n",
        "                        \n",
        "                        # Only add answers where the text can be exactly recovered from the\n",
        "                        # document. If this CAN'T happen it's likely due to weird Unicode\n",
        "                        # stuff so we will just skip the example.\n",
        "                        #\n",
        "                        # Note that this means for training mode, every example is NOT\n",
        "                        # guaranteed to be preserved.\n",
        "                        actual_text = \" \".join(doc_tokens[start_position:(end_position + 1)])\n",
        "                        cleaned_answer_text = \" \".join(\n",
        "                            whitespace_tokenize(orig_answer_text))\n",
        "                        if actual_text.find(cleaned_answer_text) == -1:\n",
        "                            logger.warning(\"Could not find answer: '%s' vs. '%s'\",\n",
        "                                           actual_text, cleaned_answer_text)\n",
        "                            continue\n",
        "                    else:\n",
        "                        start_position = -1\n",
        "                        end_position = -1\n",
        "                        orig_answer_text = \"\"\n",
        "\n",
        "                example = SquadExample(\n",
        "                    qas_id=qas_id,\n",
        "                    question_text=question_text,\n",
        "                    doc_tokens=doc_tokens,\n",
        "                    orig_answer_text=orig_answer_text,\n",
        "                    start_position=start_position,\n",
        "                    end_position=end_position,\n",
        "                    is_impossible=is_impossible)\n",
        "                examples.append(example)\n",
        "    return examples\n",
        "\n",
        "\n",
        "def convert_examples_to_features(examples, tokenizer, max_seq_length,\n",
        "                                 doc_stride, max_query_length, is_training):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    unique_id = 1000000000\n",
        "\n",
        "    features = []\n",
        "    for (example_index, example) in enumerate(examples):\n",
        "        query_tokens = tokenizer.tokenize(example.question_text)\n",
        "\n",
        "        if len(query_tokens) > max_query_length:\n",
        "            query_tokens = query_tokens[0:max_query_length]\n",
        "\n",
        "        tok_to_orig_index = []\n",
        "        orig_to_tok_index = []\n",
        "        all_doc_tokens = []\n",
        "        for (i, token) in enumerate(example.doc_tokens):\n",
        "            orig_to_tok_index.append(len(all_doc_tokens))\n",
        "            sub_tokens = tokenizer.tokenize(token)\n",
        "            for sub_token in sub_tokens:\n",
        "                tok_to_orig_index.append(i)\n",
        "                all_doc_tokens.append(sub_token)\n",
        "\n",
        "        tok_start_position = None\n",
        "        tok_end_position = None\n",
        "        if is_training and example.is_impossible:\n",
        "            tok_start_position = -1\n",
        "            tok_end_position = -1\n",
        "        if is_training and not example.is_impossible:\n",
        "            tok_start_position = orig_to_tok_index[example.start_position]\n",
        "            if example.end_position < len(example.doc_tokens) - 1:\n",
        "                tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n",
        "            else:\n",
        "                tok_end_position = len(all_doc_tokens) - 1\n",
        "            (tok_start_position, tok_end_position) = _improve_answer_span(\n",
        "                all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n",
        "                example.orig_answer_text)\n",
        "\n",
        "        # The -3 accounts for [CLS], [SEP] and [SEP]\n",
        "        max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n",
        "\n",
        "        # We can have documents that are longer than the maximum sequence length.\n",
        "        # To deal with this we do a sliding window approach, where we take chunks\n",
        "        # of the up to our max length with a stride of `doc_stride`.\n",
        "        _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "            \"DocSpan\", [\"start\", \"length\"])\n",
        "        doc_spans = []\n",
        "        start_offset = 0\n",
        "        while start_offset < len(all_doc_tokens):\n",
        "            length = len(all_doc_tokens) - start_offset\n",
        "            if length > max_tokens_for_doc:\n",
        "                length = max_tokens_for_doc\n",
        "            doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
        "            if start_offset + length == len(all_doc_tokens):\n",
        "                break\n",
        "            start_offset += min(length, doc_stride)\n",
        "\n",
        "        for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
        "            tokens = []\n",
        "            token_to_orig_map = {}\n",
        "            token_is_max_context = {}\n",
        "            segment_ids = []\n",
        "            tokens.append(\"[CLS]\")\n",
        "            segment_ids.append(0)\n",
        "            for token in query_tokens:\n",
        "                tokens.append(token)\n",
        "                segment_ids.append(0)\n",
        "            tokens.append(\"[SEP]\")\n",
        "            segment_ids.append(0)\n",
        "\n",
        "            for i in range(doc_span.length):\n",
        "                split_token_index = doc_span.start + i\n",
        "                token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n",
        "\n",
        "                is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n",
        "                                                       split_token_index)\n",
        "                token_is_max_context[len(tokens)] = is_max_context\n",
        "                tokens.append(all_doc_tokens[split_token_index])\n",
        "                segment_ids.append(1)\n",
        "            tokens.append(\"[SEP]\")\n",
        "            segment_ids.append(1)\n",
        "\n",
        "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "            # tokens are attended to.\n",
        "            input_mask = [1] * len(input_ids)\n",
        "\n",
        "            # Zero-pad up to the sequence length.\n",
        "            while len(input_ids) < max_seq_length:\n",
        "                input_ids.append(0)\n",
        "                input_mask.append(0)\n",
        "                segment_ids.append(0)\n",
        "\n",
        "            assert len(input_ids) == max_seq_length\n",
        "            assert len(input_mask) == max_seq_length\n",
        "            assert len(segment_ids) == max_seq_length\n",
        "\n",
        "            start_position = None\n",
        "            end_position = None\n",
        "            if is_training and not example.is_impossible:\n",
        "                # For training, if our document chunk does not contain an annotation\n",
        "                # we throw it out, since there is nothing to predict.\n",
        "                doc_start = doc_span.start\n",
        "                doc_end = doc_span.start + doc_span.length - 1\n",
        "                out_of_span = False\n",
        "                if not (tok_start_position >= doc_start and\n",
        "                        tok_end_position <= doc_end):\n",
        "                    out_of_span = True\n",
        "                if out_of_span:\n",
        "                    start_position = 0\n",
        "                    end_position = 0\n",
        "                else:\n",
        "                    doc_offset = len(query_tokens) + 2\n",
        "                    start_position = tok_start_position - doc_start + doc_offset\n",
        "                    end_position = tok_end_position - doc_start + doc_offset\n",
        "            if is_training and example.is_impossible:\n",
        "                start_position = 0\n",
        "                end_position = 0\n",
        "            if example_index < 20:\n",
        "                logger.info(\"*** Example ***\")\n",
        "                logger.info(\"unique_id: %s\" % (unique_id))\n",
        "                logger.info(\"example_index: %s\" % (example_index))\n",
        "                logger.info(\"doc_span_index: %s\" % (doc_span_index))\n",
        "                logger.info(\"tokens: %s\" % \" \".join(tokens))\n",
        "                logger.info(\"token_to_orig_map: %s\" % \" \".join([\n",
        "                    \"%d:%d\" % (x, y) for (x, y) in token_to_orig_map.items()]))\n",
        "                logger.info(\"token_is_max_context: %s\" % \" \".join([\n",
        "                    \"%d:%s\" % (x, y) for (x, y) in token_is_max_context.items()\n",
        "                ]))\n",
        "                logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "                logger.info(\n",
        "                    \"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "                logger.info(\n",
        "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "                if is_training and example.is_impossible:\n",
        "                    logger.info(\"impossible example\")\n",
        "                if is_training and not example.is_impossible:\n",
        "                    answer_text = \" \".join(tokens[start_position:(end_position + 1)])\n",
        "                    logger.info(\"start_position: %d\" % (start_position))\n",
        "                    logger.info(\"end_position: %d\" % (end_position))\n",
        "                    logger.info(\n",
        "                        \"answer: %s\" % (answer_text))\n",
        "\n",
        "            features.append(\n",
        "                InputFeatures(\n",
        "                    unique_id=unique_id,\n",
        "                    example_index=example_index,\n",
        "                    doc_span_index=doc_span_index,\n",
        "                    tokens=tokens,\n",
        "                    token_to_orig_map=token_to_orig_map,\n",
        "                    token_is_max_context=token_is_max_context,\n",
        "                    input_ids=input_ids,\n",
        "                    input_mask=input_mask,\n",
        "                    segment_ids=segment_ids,\n",
        "                    start_position=start_position,\n",
        "                    end_position=end_position,\n",
        "                    is_impossible=example.is_impossible))\n",
        "            unique_id += 1\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n",
        "                         orig_answer_text):\n",
        "    \"\"\"Returns tokenized answer spans that better match the annotated answer.\"\"\"\n",
        "\n",
        "    # The SQuAD annotations are character based. We first project them to\n",
        "    # whitespace-tokenized words. But then after WordPiece tokenization, we can\n",
        "    # often find a \"better match\". For example:\n",
        "    #\n",
        "    #   Question: What year was John Smith born?\n",
        "    #   Context: The leader was John Smith (1895-1943).\n",
        "    #   Answer: 1895\n",
        "    #\n",
        "    # The original whitespace-tokenized answer will be \"(1895-1943).\". However\n",
        "    # after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match\n",
        "    # the exact answer, 1895.\n",
        "    #\n",
        "    # However, this is not always possible. Consider the following:\n",
        "    #\n",
        "    #   Question: What country is the top exporter of electornics?\n",
        "    #   Context: The Japanese electronics industry is the lagest in the world.\n",
        "    #   Answer: Japan\n",
        "    #\n",
        "    # In this case, the annotator chose \"Japan\" as a character sub-span of\n",
        "    # the word \"Japanese\". Since our WordPiece tokenizer does not split\n",
        "    # \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare\n",
        "    # in SQuAD, but does happen.\n",
        "    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n",
        "\n",
        "    for new_start in range(input_start, input_end + 1):\n",
        "        for new_end in range(input_end, new_start - 1, -1):\n",
        "            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n",
        "            if text_span == tok_answer_text:\n",
        "                return (new_start, new_end)\n",
        "\n",
        "    return (input_start, input_end)\n",
        "\n",
        "\n",
        "def _check_is_max_context(doc_spans, cur_span_index, position):\n",
        "    \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n",
        "\n",
        "    # Because of the sliding window approach taken to scoring documents, a single\n",
        "    # token can appear in multiple documents. E.g.\n",
        "    #  Doc: the man went to the store and bought a gallon of milk\n",
        "    #  Span A: the man went to the\n",
        "    #  Span B: to the store and bought\n",
        "    #  Span C: and bought a gallon of\n",
        "    #  ...\n",
        "    #\n",
        "    # Now the word 'bought' will have two scores from spans B and C. We only\n",
        "    # want to consider the score with \"maximum context\", which we define as\n",
        "    # the *minimum* of its left and right context (the *sum* of left and\n",
        "    # right context will always be the same, of course).\n",
        "    #\n",
        "    # In the example the maximum context for 'bought' would be span C since\n",
        "    # it has 1 left context and 3 right context, while span B has 4 left context\n",
        "    # and 0 right context.\n",
        "    best_score = None\n",
        "    best_span_index = None\n",
        "    for (span_index, doc_span) in enumerate(doc_spans):\n",
        "        end = doc_span.start + doc_span.length - 1\n",
        "        if position < doc_span.start:\n",
        "            continue\n",
        "        if position > end:\n",
        "            continue\n",
        "        num_left_context = position - doc_span.start\n",
        "        num_right_context = end - position\n",
        "        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n",
        "        if best_score is None or score > best_score:\n",
        "            best_score = score\n",
        "            best_span_index = span_index\n",
        "\n",
        "    return cur_span_index == best_span_index\n",
        "\n",
        "\n",
        "RawResult = collections.namedtuple(\"RawResult\",\n",
        "                                   [\"unique_id\", \"start_logits\", \"end_logits\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvJdvX-aPuRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "do_train = True # Are we training?\n",
        "local_rank = -1  \n",
        "version_2_with_negative=False # Version 2 of SQuAD has \"impossible questions\" to consider\n",
        "do_lower_case = True # Do we need to force data to lowercase?\n",
        "max_seq_length=324 # How long a sequence do we consider? If sequence is bigger, truncate\n",
        "doc_stride=128 # How much of each sequence to consider at once\n",
        "max_query_length=64\n",
        "train_batch_size=32\n",
        "gradient_accumulation_steps=1\n",
        "num_train_epochs=1.0\n",
        "learning_rate=5e-5\n",
        "loss_scale=0\n",
        "warmup_proportion=0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-_q_hkQtf2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget 'https://drive.google.com/uc?id=1w0yYOeytZOFv_SuydI5xS_pjlFLd--bT' train-v1.1_aisc_extract"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "acMENJjbJbk-",
        "colab": {}
      },
      "source": [
        "# Transform examples into input features\n",
        "print(\"converting features\")\n",
        "cached_train_features_file = 'train-v1.1'+'_aisc_extract'\n",
        "try:\n",
        "    with open(\"/content/gdrive/My Drive/\" + cached_train_features_file, \"rb\") as reader:\n",
        "        train_features = pickle.load(reader)\n",
        "except:\n",
        "  print(\"failed\")\n",
        "  train_features = convert_examples_to_features(\n",
        "        examples=train_examples,\n",
        "        tokenizer=tokenizer,\n",
        "        max_seq_length=max_seq_length,\n",
        "        max_query_length=max_query_length,\n",
        "        is_training=True)\n",
        "\n",
        "  logger.info(\"  Saving train features into cached file %s\", cached_train_features_file)\n",
        "  with open(cached_train_features_file, \"wb\") as writer:\n",
        "    pickle.dump(train_features, writer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54vMMs-2GEmY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
        "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
        "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
        "all_start_positions = torch.tensor([f.start_position for f in train_features], dtype=torch.long)\n",
        "all_end_positions = torch.tensor([f.end_position for f in train_features], dtype=torch.long)\n",
        "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
        "                           all_start_positions, all_end_positions)\n",
        "\n",
        "print(\"setting up data loader\")\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n",
        "num_train_optimization_steps = len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n",
        "\n",
        "# Prepare optimizer\n",
        "param_optimizer = list(model.named_parameters())\n",
        "\n",
        "# hack to remove pooler, which is not used\n",
        "# thus it produce None grad that break apex\n",
        "param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                     lr=learning_rate,\n",
        "                     warmup=warmup_proportion,\n",
        "                     t_total=num_train_optimization_steps)\n",
        "\n",
        "global_step = 0\n",
        "logger.setLevel(0)\n",
        "print(\"***** Running training *****\")\n",
        "print(f\"  Num orig examples = {len(train_examples)}\", len(train_examples))\n",
        "print(\"  Num split examples = %d\", len(train_features))\n",
        "print(\"  Batch size = %d\", train_batch_size)\n",
        "print(\"  Num steps = %d\", num_train_optimization_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlISD4kxPq1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"training\")\n",
        "model.train()\n",
        "for epoch in trange(int(num_train_epochs), desc=\"Epoch\"):\n",
        "    for step, batch in enumerate(tqdm_notebook(train_dataloader, desc=\"Iteration\", disable=local_rank not in [-1, 0])):\n",
        "        if n_gpu == 1:\n",
        "            batch = tuple(t.to(device) for t in batch) # multi-gpu does scattering it-self\n",
        "        input_ids, input_mask, segment_ids, start_positions, end_positions = batch\n",
        "        loss = model(input_ids, segment_ids, input_mask, start_positions, end_positions)\n",
        "        if n_gpu > 1:\n",
        "            loss = loss.mean() # mean() to average on multi-gpu.\n",
        "        if gradient_accumulation_steps > 1:\n",
        "            loss = loss / gradient_accumulation_steps\n",
        "\n",
        "        loss.backward()\n",
        "        if (step + 1) % gradient_accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            global_step += 1"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}